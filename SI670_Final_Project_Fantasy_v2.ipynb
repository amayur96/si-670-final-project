{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fantasy Football Quantile Predictions (v2)\n",
        "\n",
        "This notebook uses the improved model from `SI670_Final_Project_Model_v4` with more sophisticated features:\n",
        "- **Volatility features**: rolling std, MAE, deviation from average\n",
        "- **Workload features**: play share, target share\n",
        "- **Opponent features**: defensive EPA allowed\n",
        "- **Spike rate**: Probability of boom games\n",
        "\n",
        "## What You'll Get\n",
        "- **pred_10 (Floor)**: Safe minimum - player should score above this 90% of the time\n",
        "- **pred_50 (Median)**: Expected score - most likely outcome\n",
        "- **pred_90 (Ceiling)**: Upside - player could score this high 10% of the time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Connect to ESPN League\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to league. Your team: Math Guys\n"
          ]
        }
      ],
      "source": [
        "# Connect to ESPN Fantasy League\n",
        "from espn_api.football import League\n",
        "import nfl_data_py as nfl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "league = League(\n",
        "    league_id=567575, \n",
        "    year=2025, \n",
        "    espn_s2='AECVN3FcAWfB56xxM5SnNVqsxq9soOxMmzDH1CfYYOkX3KIrzeGSsTMZ0CwJLPQoBYxLMp59ILoZ0CvUnTrBbU15b2PwD1v9fZRoO5iMb%2Fy%2FWPaOqPwlwSx2ShvBAt%2BSqJxtboHzcpSuSOgASzSNx4divXOEc4aVZjnOx7qRJ9YbE800NnLCNiLMBpaHjdZg%2BMN6vwCInJrKejPDXsmdjo%2FIkV0IfCLQHr6QHyJjLhqOwAPozqNPyGa1ZZT8DOxA%2BmpTsa5v9cgfJ4V%2BVZzxzr95KxBS0k%2BYJMt7OWSdA%2B2yUQ%3D%3D', \n",
        "    swid='{280AA84B-DE12-4B3D-80F2-283BF634242B}'\n",
        ")\n",
        "team = league.teams[9]  # <-- Change index to select your team\n",
        "print(f\"Connected to league. Your team: {team.team_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Data & Build Features\n",
        "\n",
        "Using the Model_v4 feature engineering approach with:\n",
        "- Player time-series & volatility features\n",
        "- Workload/opportunity features  \n",
        "- Opponent defensive features\n",
        "- Spike (boom game) probability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Half-PPR scoring rules\n",
        "scoring_rules = {\n",
        "    'pass_yd': 0.04, 'pass_td': 4,\n",
        "    'rush_yd': 0.1, 'rush_td': 6,\n",
        "    'rec_yd': 0.1, 'rec_td': 6,\n",
        "    'rec': 0.5,\n",
        "    'int': -2, 'fumble_lost': -2,\n",
        "    'qb_kneel_yd': -0.1\n",
        "}\n",
        "\n",
        "def calculate_fantasy_points_per_play(df: pd.DataFrame, scoring_rules: dict) -> pd.DataFrame:\n",
        "    \"\"\"Calculates all fantasy points earned on a single play based on half-PPR rules.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Passing Points (QB)\n",
        "    df['fp_pass'] = (df['passing_yards'].fillna(0) * scoring_rules['pass_yd']) + \\\n",
        "                    (df['pass_touchdown'].fillna(0) * scoring_rules['pass_td'])\n",
        "    \n",
        "    # Rushing Points (RB/QB)\n",
        "    df['fp_rush'] = (df['rushing_yards'].fillna(0) * scoring_rules['rush_yd']) + \\\n",
        "                    (df['rush_touchdown'].fillna(0) * scoring_rules['rush_td'])\n",
        "    \n",
        "    # Receiving Points - Calculate conditionally to prevent leakage\n",
        "    is_reception = df['complete_pass'].fillna(0) == 1\n",
        "    df['fp_rec'] = np.where(is_reception, \n",
        "        (df['receiving_yards'].fillna(0) * scoring_rules['rec_yd']) + \\\n",
        "        (df['pass_touchdown'].fillna(0) * scoring_rules['rec_td']) + \\\n",
        "        (df['complete_pass'].fillna(0) * scoring_rules['rec']),\n",
        "        0\n",
        "    )\n",
        "    \n",
        "    # Interception (for primary passer)\n",
        "    df['fp_int'] = df['interception'].fillna(0) * scoring_rules['int']\n",
        "    \n",
        "    # Fumble Lost (for primary ball carrier/or receiver)\n",
        "    df['fp_fumble_lost'] = df['fumble_lost'].fillna(0) * scoring_rules['fumble_lost']\n",
        "    df['fp_fumble_penalty'] = df['fp_fumble_lost'].fillna(0)  # Separate column for attribution\n",
        "    \n",
        "    # QB Kneel Adjustment\n",
        "    df['fp_kneel'] = np.where(df['play_type'] == 'qb_kneel',\n",
        "                              df['yards_gained'].fillna(0) * scoring_rules['qb_kneel_yd'], 0)\n",
        "    return df\n",
        "\n",
        "def calculate_weekly_fantasy_points_final(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Aggregates all fantasy points earned by a player across a week.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Calculate the total points for each respective player id\n",
        "    df['fp_pass_total'] = df['fp_pass'].fillna(0) + df['fp_int'].fillna(0) + df['fp_kneel'].fillna(0)\n",
        "    df['fp_rush_total'] = df['fp_rush'].fillna(0)\n",
        "    df['fp_rec_total'] = df['fp_rec'].fillna(0)\n",
        "\n",
        "    id_vars = ['season', 'week']\n",
        "    \n",
        "    contributions_map = {\n",
        "        'passer_player_id': 'fp_pass_total',\n",
        "        'rusher_player_id': 'fp_rush_total',\n",
        "        'receiver_player_id': 'fp_rec_total',\n",
        "        \n",
        "        # Apply Fumble PENALTY (Negative Points) to the three primary roles \n",
        "        # to ensure the correct player is penalized for losing the ball.\n",
        "        'passer_player_id_fumble': 'fp_fumble_penalty', \n",
        "        'rusher_player_id_fumble': 'fp_fumble_penalty',\n",
        "        'receiver_player_id_fumble': 'fp_fumble_penalty',\n",
        "    }\n",
        "\n",
        "    contributions = []\n",
        "    for id_col_raw, point_col in contributions_map.items():\n",
        "        id_col = id_col_raw.replace('_fumble', '')\n",
        "        \n",
        "        temp_df = df.rename(columns={id_col: 'player_id', point_col: 'points'})\n",
        "        contributions.append(temp_df[id_vars + ['player_id', 'points']])\n",
        "\n",
        "    # Stack contributions and drop any missing player ids\n",
        "    df_all_points = pd.concat(contributions, ignore_index=True)\n",
        "    df_all_points.dropna(subset=['player_id'], inplace=True)\n",
        "\n",
        "    # Group by week and season for each respective player id and return the table\n",
        "    df_target_Y = df_all_points.groupby(id_vars + ['player_id'])['points'].sum().reset_index()\n",
        "    \n",
        "    return df_target_Y.rename(columns={'points': 'Y_target_points'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading NFL play-by-play data (this may take a minute)...\n",
            "2015 done.\n",
            "2016 done.\n",
            "2017 done.\n",
            "2018 done.\n",
            "2019 done.\n",
            "2020 done.\n",
            "2021 done.\n",
            "2022 done.\n",
            "2023 done.\n",
            "2024 done.\n",
            "Downcasting floats.\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"['player_id'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m df_mdl1 = calculate_fantasy_points_per_play(df_mdl0, scoring_rules)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Weekly target Y per player\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m df_target_Y = \u001b[43mcalculate_weekly_fantasy_points_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_mdl1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget variable created. Total player-weeks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_target_Y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mcalculate_weekly_fantasy_points_final\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m id_col, point_col \u001b[38;5;129;01min\u001b[39;00m contributions_map.items():\n\u001b[32m     42\u001b[39m     tmp = df.rename(columns={id_col: \u001b[33m'\u001b[39m\u001b[33mplayer_id\u001b[39m\u001b[33m'\u001b[39m, point_col: \u001b[33m'\u001b[39m\u001b[33mpoints\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     contributions.append(\u001b[43mtmp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mid_vars\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mplayer_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpoints\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     45\u001b[39m df_all_points = pd.concat(contributions, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     46\u001b[39m df_all_points.dropna(subset=[\u001b[33m'\u001b[39m\u001b[33mplayer_id\u001b[39m\u001b[33m'\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/si-670-final-project/.venv-312/lib/python3.12/site-packages/pandas/core/frame.py:3813\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   3812\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m3813\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   3815\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   3816\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/si-670-final-project/.venv-312/lib/python3.12/site-packages/pandas/core/indexes/base.py:6070\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6067\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6068\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6070\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6072\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6074\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/si-670-final-project/.venv-312/lib/python3.12/site-packages/pandas/core/indexes/base.py:6133\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6132\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6133\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"['player_id'] not in index\""
          ]
        }
      ],
      "source": [
        "# Load NFL play-by-play data (2015-2024 for richer history like Model_v4)\n",
        "print(\"Loading NFL play-by-play data (this may take a minute)...\")\n",
        "df = nfl.import_pbp_data([2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024])\n",
        "\n",
        "# Keep fantasy-relevant plays\n",
        "fantasy_play_types = ['pass', 'run', 'qb_kneel']\n",
        "exclude_play_types = ['no_play', 'qb_spike', 'field_goal', 'extra_point', 'punt', 'kickoff']\n",
        "\n",
        "df_fantasy_plays = df[\n",
        "    df['play_type'].isin(fantasy_play_types) & ~df['play_type'].isin(exclude_play_types)\n",
        "].copy()\n",
        "\n",
        "# Fix: Impute Rusher ID for QB Kneels\n",
        "df_fantasy_plays['rusher_player_id'] = np.where(\n",
        "    (df_fantasy_plays['play_type'] == 'qb_kneel') & \n",
        "    df_fantasy_plays['rusher_player_id'].isnull(), \n",
        "    df_fantasy_plays['passer_player_id'], \n",
        "    df_fantasy_plays['rusher_player_id']\n",
        ")\n",
        "\n",
        "# Filter out penalties and 2-point attempts\n",
        "df_mdl1 = df_fantasy_plays[\n",
        "    (df_fantasy_plays['penalty'] == 0) & \n",
        "    (df_fantasy_plays['two_point_attempt'] == 0)\n",
        "].copy()\n",
        "\n",
        "# Select modeling columns\n",
        "mdl_cols = [\n",
        "    'game_id', 'play_id', 'season', 'week', 'posteam', 'defteam',\n",
        "    'passer_player_id', 'rusher_player_id', 'receiver_player_id',\n",
        "    'passing_yards', 'pass_touchdown', 'interception',\n",
        "    'rushing_yards', 'rush_touchdown', 'fumble_lost',\n",
        "    'receiving_yards', 'complete_pass',\n",
        "    'play_type', 'yards_gained', 'epa', 'cpoe'\n",
        "]\n",
        "\n",
        "df_mdl0 = df_mdl1[mdl_cols].copy()\n",
        "\n",
        "# Calculate per-play fantasy points\n",
        "df_mdl1 = calculate_fantasy_points_per_play(df_mdl0, scoring_rules)\n",
        "\n",
        "# Weekly target Y per player\n",
        "df_target_Y = calculate_weekly_fantasy_points_final(df_mdl1)\n",
        "print(f\"Target variable created. Total player-weeks: {len(df_target_Y)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BLOCK 1: Feature Aggregation & Position Inference (from Model_v4)\n",
        "\n",
        "feature_agg_rules = {\n",
        "    'passing_yards': 'sum',\n",
        "    'rushing_yards': 'sum',\n",
        "    'receiving_yards': 'sum',\n",
        "    'pass_touchdown': 'sum',\n",
        "    'rush_touchdown': 'sum',\n",
        "    'interception': 'sum',\n",
        "    'complete_pass': 'sum',\n",
        "    'epa': 'mean',\n",
        "    'cpoe': 'mean',\n",
        "}\n",
        "\n",
        "id_vars = ['season', 'week']\n",
        "feature_cols = list(feature_agg_rules.keys())\n",
        "context_cols = ['posteam', 'defteam']\n",
        "\n",
        "df_select = df_mdl1[id_vars + feature_cols + context_cols + \n",
        "    ['passer_player_id', 'rusher_player_id', 'receiver_player_id']].copy()\n",
        "\n",
        "df_X_long = pd.melt(\n",
        "    df_select,\n",
        "    id_vars=id_vars + feature_cols + context_cols,\n",
        "    value_vars=['passer_player_id', 'rusher_player_id', 'receiver_player_id'],\n",
        "    var_name='role_type',\n",
        "    value_name='player_id'\n",
        ")\n",
        "df_X_long.dropna(subset=['player_id'], inplace=True)\n",
        "\n",
        "group_keys = ['season', 'week', 'player_id', 'posteam', 'defteam']\n",
        "df_features_X = df_X_long.groupby(group_keys).agg(feature_agg_rules).reset_index()\n",
        "\n",
        "df_counts = df_X_long.groupby(group_keys).size().reset_index(name='total_plays_involved')\n",
        "df_features_X = pd.merge(df_features_X, df_counts, on=group_keys, how='left')\n",
        "\n",
        "# Position Inference\n",
        "role_counts = df_X_long.groupby(['player_id', 'role_type']).size().reset_index(name='count')\n",
        "player_role_summary = role_counts.pivot(index='player_id', columns='role_type', values='count').fillna(0)\n",
        "\n",
        "def infer_position(row):\n",
        "    pass_count = row.get('passer_player_id', 0)\n",
        "    rush_count = row.get('rusher_player_id', 0)\n",
        "    rec_count = row.get('receiver_player_id', 0)\n",
        "    if pass_count >= 50:\n",
        "        return 'QB'\n",
        "    if rush_count >= 10 and rush_count > rec_count * 2:\n",
        "        return 'RB'\n",
        "    if rec_count >= 10:\n",
        "        return 'WR/TE'\n",
        "    return 'OTH'\n",
        "\n",
        "player_role_summary['position'] = player_role_summary.apply(infer_position, axis=1)\n",
        "player_position_map = player_role_summary['position']\n",
        "\n",
        "print(\"Block 1: Aggregation & Position Inference Complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BLOCK 2: Feature Engineering (Lagged, Volatility, Opponent) - from Model_v4\n",
        "\n",
        "# Merge features with target\n",
        "df_final = pd.merge(\n",
        "    df_features_X, \n",
        "    df_target_Y,    \n",
        "    on=['season', 'week', 'player_id'],\n",
        "    how='left'\n",
        ")\n",
        "df_final['Y_target_points'] = df_final['Y_target_points'].fillna(0)\n",
        "df_final.sort_values(by=['player_id', 'season', 'week'], inplace=True)\n",
        "\n",
        "# Add Player Opportunity Share (Workload Metric)\n",
        "team_opportunities = df_final.groupby(['season', 'week', 'posteam'])['total_plays_involved'].sum().rename('team_total_plays').reset_index()\n",
        "df_final = df_final.merge(team_opportunities, on=['season', 'week', 'posteam'], how='left')\n",
        "df_final['play_share'] = df_final['total_plays_involved'] / df_final['team_total_plays']\n",
        "\n",
        "# --- A. PLAYER TIME-SERIES & VOLATILITY FEATURES ---\n",
        "df_final['Y_lag_1'] = df_final.groupby('player_id')['Y_target_points'].shift(1)\n",
        "df_final['Y_roll_avg_3'] = df_final.groupby('player_id')['Y_target_points'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=3, min_periods=1).mean()\n",
        ")\n",
        "df_final['Y_cum_avg'] = df_final.groupby('player_id')['Y_target_points'].transform(\n",
        "    lambda x: x.shift(1).expanding(min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Volatility (Risk Metrics)\n",
        "df_final['Y_roll_std_3'] = df_final.groupby('player_id')['Y_target_points'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=3, min_periods=1).std()\n",
        ")\n",
        "df_final['Y_MAE_roll_3'] = df_final.groupby('player_id')['Y_target_points'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=3, min_periods=1).apply(lambda y: np.mean(np.abs(y - np.mean(y))), raw=True)\n",
        ")\n",
        "df_final['Y_cum_std'] = df_final.groupby('player_id')['Y_target_points'].transform(\n",
        "    lambda x: x.shift(1).expanding(min_periods=1).std() \n",
        ")\n",
        "df_final['play_share_roll_std_3'] = df_final.groupby('player_id')['play_share'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=3, min_periods=1).std()\n",
        ")\n",
        "\n",
        "# Deviation from Cumulative Average (regime change signal)\n",
        "df_final['Y_dev_from_cum_avg'] = df_final['Y_roll_avg_3'] - df_final['Y_cum_avg']\n",
        "\n",
        "# --- B. OPPONENT EFFICIENCY FEATURES ---\n",
        "print(\"Calculating Defensive Performance Metrics...\")\n",
        "\n",
        "df_defense_metrics = df_mdl1[['season', 'week', 'defteam', 'epa']].copy()\n",
        "df_defense_metrics.rename(columns={'defteam': 'defense_team_id'}, inplace=True)\n",
        "\n",
        "df_defense_agg = df_defense_metrics.groupby(['season', 'week', 'defense_team_id']).agg(\n",
        "    epa_allowed_game=('epa', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "df_defense_agg.sort_values(by=['defense_team_id', 'season', 'week'], inplace=True)\n",
        "df_defense_agg['Opp_EPA_Mean'] = df_defense_agg.groupby('defense_team_id')['epa_allowed_game'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=5, min_periods=1).mean()\n",
        ")\n",
        "\n",
        "df_def_stats_merge = df_defense_agg[['season', 'week', 'defense_team_id', 'Opp_EPA_Mean']].copy()\n",
        "\n",
        "# Merge Defensive Stats\n",
        "df_final = pd.merge(\n",
        "    df_final,\n",
        "    df_def_stats_merge,\n",
        "    left_on=['season', 'week', 'defteam'], \n",
        "    right_on=['season', 'week', 'defense_team_id'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# --- C. TARGET SHARE ---\n",
        "team_pass_attempts = df_mdl1[df_mdl1['play_type'] == 'pass'].copy()\n",
        "team_pass_attempts = team_pass_attempts.groupby(['season', 'week', 'posteam']).size().reset_index(name='Team_Pass_Attempts')\n",
        "df_final = pd.merge(df_final, team_pass_attempts, on=['season', 'week', 'posteam'], how='left')\n",
        "df_final['Team_Pass_Attempts'] = df_final['Team_Pass_Attempts'].fillna(0)\n",
        "\n",
        "df_final['Target_Share'] = np.where(\n",
        "    df_final['Team_Pass_Attempts'] > 0, \n",
        "    df_final['complete_pass'] / df_final['Team_Pass_Attempts'], \n",
        "    0\n",
        ")\n",
        "df_final['Target_Share_lag_1'] = df_final.groupby('player_id')['Target_Share'].shift(1)\n",
        "\n",
        "# --- D. SPIKE RATE (Boom Game Probability) ---\n",
        "SPIKE_THRESHOLD = 20.0\n",
        "df_final['is_spike'] = (df_final['Y_target_points'] >= SPIKE_THRESHOLD).astype(int)\n",
        "df_final['Y_Spike_Rate_5'] = df_final.groupby('player_id')['is_spike'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=5, min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# --- D. FINAL CLEANUP ---\n",
        "df_final.drop(columns=['defense_team_id', 'team_total_plays', 'Team_Pass_Attempts', 'Target_Share'], inplace=True)\n",
        "\n",
        "# Fill NaNs for all new features (first few weeks/first few games)\n",
        "new_features_to_fill = [col for col in df_final.columns if col not in group_keys + ['Y_target_points']]\n",
        "df_final[new_features_to_fill] = df_final[new_features_to_fill].fillna(0)\n",
        "\n",
        "print(\"\\nBlock 2: Feature Engineering Complete. df_final is ready.\")\n",
        "print(f\"df_final shape: {df_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Quantile Models\n",
        "\n",
        "Using the Model_v4 approach with:\n",
        "- Leakage removal (no raw game stats in X)\n",
        "- OneHotEncoder for categorical features\n",
        "- HistGradientBoostingRegressor for quantiles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "id_cols = ['season', 'week', 'player_id']\n",
        "target_col = 'Y_target_points'\n",
        "\n",
        "# Features that leak the answer (raw game stats)\n",
        "LEAKAGE_FEATURES = [\n",
        "    'passing_yards', 'rushing_yards', 'receiving_yards', 'pass_touchdown', \n",
        "    'rush_touchdown', 'interception', 'complete_pass', \n",
        "    'epa', 'cpoe',  # Also remove - strong proxies for outcome\n",
        "]\n",
        "\n",
        "# Create df_features, removing leakage and target\n",
        "df_features = df_final.drop(columns=[target_col] + LEAKAGE_FEATURES)\n",
        "\n",
        "# Add position as feature\n",
        "df_features['position'] = df_features['player_id'].map(player_position_map)\n",
        "df_features['position'] = df_features['position'].fillna('OTH')\n",
        "\n",
        "# Drop ID columns for feature matrix\n",
        "FINAL_ID_COLUMNS_TO_DROP = ['season', 'week', 'player_id']\n",
        "X = df_features.drop(columns=FINAL_ID_COLUMNS_TO_DROP)\n",
        "\n",
        "# Define feature types\n",
        "categorical_features = ['posteam', 'defteam', 'position']\n",
        "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "y = df_final[target_col]\n",
        "\n",
        "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
        "print(f\"Categorical features: {categorical_features}\")\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
        "        ('num', StandardScaler(), numerical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Chronological split (90/10)\n",
        "split_point = int(len(X) * 0.90)\n",
        "X_train = X.iloc[:split_point]\n",
        "X_test = X.iloc[split_point:]\n",
        "y_train = y.iloc[:split_point]\n",
        "y_test = y.iloc[split_point:]\n",
        "ids_test = df_final[id_cols].iloc[split_point:]\n",
        "\n",
        "print(f\"\\nTrain size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "\n",
        "# Transform data\n",
        "X_train_scaled = preprocessor.fit_transform(X_train)\n",
        "X_test_scaled = preprocessor.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Quantile Models\n",
        "quantiles = [0.10, 0.50, 0.90]\n",
        "y_preds = {}\n",
        "models = {}\n",
        "\n",
        "print(\"--- Training Quantile Models (Model_v4 Features) ---\")\n",
        "for q in quantiles:\n",
        "    print(f\"Training quantile {q}\")\n",
        "    model = HistGradientBoostingRegressor(\n",
        "        loss='quantile',\n",
        "        quantile=q,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.01,\n",
        "        max_iter=2000,\n",
        "        random_state=321\n",
        "    )\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_preds[q] = model.predict(X_test_scaled)\n",
        "    models[q] = model\n",
        "print(\"Finished training.\\n\")\n",
        "\n",
        "# Evaluate\n",
        "def pinball_loss(y_true, y_pred, q):\n",
        "    err = y_true - y_pred\n",
        "    return np.mean(np.where(err >= 0, q * err, (1 - q) * -err))\n",
        "\n",
        "df_predictions = pd.DataFrame({\n",
        "    'y_test': y_test.values,\n",
        "    'pred_10': y_preds[0.10],\n",
        "    'pred_50': y_preds[0.50],\n",
        "    'pred_90': y_preds[0.90]\n",
        "}, index=y_test.index)\n",
        "df_predictions = df_predictions.join(ids_test)\n",
        "df_predictions['position'] = df_predictions['player_id'].map(player_position_map)\n",
        "df_predictions['position'] = df_predictions['position'].fillna('OTH')\n",
        "\n",
        "mae = mean_absolute_error(df_predictions['y_test'], df_predictions['pred_50'])\n",
        "coverage = ((df_predictions['y_test'] >= df_predictions['pred_10']) & \n",
        "            (df_predictions['y_test'] <= df_predictions['pred_90'])).mean() * 100\n",
        "pb10 = pinball_loss(df_predictions['y_test'], df_predictions['pred_10'], 0.10)\n",
        "\n",
        "print(\"--- Evaluation Summary (Model_v4 Features) ---\")\n",
        "print(f\"MAE (Median Prediction): {mae:.2f}\")\n",
        "print(f\"80% PI Coverage Actual: {coverage:.2f}%\")\n",
        "print(f\"Pinball Loss (10th Quantile): {pb10:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare for Roster Predictions\n",
        "\n",
        "Build player name mapping and get latest historical features for each player.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Get player ID crosswalk\n",
        "ids = nfl.import_ids()\n",
        "name_col_ids = 'name' if 'name' in ids.columns else 'full_name'\n",
        "ids_map = ids[['gsis_id', name_col_ids]].dropna().drop_duplicates('gsis_id')\n",
        "ids_map = ids_map.rename(columns={'gsis_id': 'player_id', name_col_ids: 'player_name'})\n",
        "\n",
        "# Name normalization\n",
        "SUFFIXES = {'jr','sr','ii','iii','iv','v'}\n",
        "def normalize_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return ''\n",
        "    s = unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore').decode('ASCII')\n",
        "    s = re.sub(r\"[^a-zA-Z\\s]\", \"\", s).strip().lower()\n",
        "    parts = [p for p in s.split() if p and p not in SUFFIXES]\n",
        "    return \" \".join(parts)\n",
        "\n",
        "ids_map['name_norm'] = ids_map['player_name'].apply(normalize_name)\n",
        "\n",
        "# Get latest season/week\n",
        "latest_season = df_final['season'].max()\n",
        "df_latest_season = df_final[df_final['season'] == latest_season].copy()\n",
        "latest_week_in_data = int(df_latest_season['week'].max())\n",
        "\n",
        "print(f\"Latest data: {latest_season} week {latest_week_in_data}\")\n",
        "\n",
        "# Get each player's latest features (including position)\n",
        "df_player_hist = df_latest_season.sort_values(['player_id', 'week']).groupby('player_id').last().reset_index()\n",
        "df_player_hist['position'] = df_player_hist['player_id'].map(player_position_map).fillna('OTH')\n",
        "df_player_hist = df_player_hist.merge(ids_map[['player_id', 'player_name', 'name_norm']], on='player_id', how='left')\n",
        "\n",
        "print(f\"Players with historical data: {len(df_player_hist)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Your Roster Predictions\n",
        "\n",
        "Generate quantile predictions for your ESPN roster using:\n",
        "- **Historical patterns** (lagged values, volatility, spike rate)\n",
        "- **Workload features** (play share, target share)\n",
        "- **Opponent context** (defensive EPA)\n",
        "\n",
        "**How to use these predictions**:\n",
        "- **Need a safe floor?** Start players with high `pred_10` values\n",
        "- **Need upside?** Start players with high `pred_90` values\n",
        "- **Best overall?** Sort by `pred_50` (median expected points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get ESPN roster predictions\n",
        "\n",
        "# Find available weeks with projections\n",
        "sample_stats = getattr(team.roster[0], 'stats', {})\n",
        "available_weeks = sorted([w for w in sample_stats.keys() if w > 0 and 'projected_points' in sample_stats.get(w, {})])\n",
        "print(f\"Available ESPN projection weeks: {available_weeks}\")\n",
        "\n",
        "# Weeks to predict\n",
        "weeks_to_predict = [13, 14, 15]\n",
        "\n",
        "# Features used by model (must match training)\n",
        "print(f\"\\nUsing {len(numerical_features)} features: {numerical_features[:10]}...\" if len(numerical_features) > 10 else f\"Using features: {numerical_features}\")\n",
        "\n",
        "for pred_week in weeks_to_predict:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"WEEK {pred_week} PREDICTIONS ({latest_season} Season)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Get ESPN roster info\n",
        "    roster_data = []\n",
        "    for p in team.roster:\n",
        "        name = getattr(p, 'name', None)\n",
        "        if name and 'D/ST' not in name:\n",
        "            stats = getattr(p, 'stats', {}) or {}\n",
        "            week_data = stats.get(pred_week, {})\n",
        "            espn_weekly_proj = week_data.get('projected_points', 0) or 0\n",
        "            actual_pts = week_data.get('points', None)\n",
        "            roster_data.append({\n",
        "                'player_name_espn': name,\n",
        "                'espn_proj_pts': espn_weekly_proj,\n",
        "                'actual_pts': actual_pts,\n",
        "            })\n",
        "    \n",
        "    espn_df = pd.DataFrame(roster_data)\n",
        "    espn_df['name_norm'] = espn_df['player_name_espn'].apply(normalize_name)\n",
        "    \n",
        "    # Get lag data from week before prediction\n",
        "    lag_week = pred_week - 1\n",
        "    df_lag_data = df_latest_season[df_latest_season['week'] == lag_week].copy()\n",
        "    df_lag_data['position'] = df_lag_data['player_id'].map(player_position_map).fillna('OTH')\n",
        "    df_lag_data = df_lag_data.merge(ids_map[['player_id', 'player_name', 'name_norm']], on='player_id', how='left')\n",
        "    \n",
        "    if len(df_lag_data) == 0:\n",
        "        print(f\"  Note: No week {lag_week} data, using most recent available data\")\n",
        "        df_lag_data = df_player_hist.copy()\n",
        "    \n",
        "    # Match ESPN roster to NFL data\n",
        "    lag_cols = ['player_id', 'name_norm', 'posteam', 'defteam', 'position'] + numerical_features\n",
        "    available_cols = [c for c in lag_cols if c in df_lag_data.columns]\n",
        "    merged = espn_df.merge(df_lag_data[available_cols], on='name_norm', how='left')\n",
        "    \n",
        "    matched = merged[merged['player_id'].notna()].copy()\n",
        "    unmatched = merged[merged['player_id'].isna()]['player_name_espn'].tolist()\n",
        "    \n",
        "    print(f\"Matched {len(matched)} / {len(espn_df)} roster players\")\n",
        "    if unmatched:\n",
        "        print(f\"Unmatched: {unmatched}\")\n",
        "    \n",
        "    if len(matched) == 0:\n",
        "        print(\"No players matched, skipping this week.\")\n",
        "        continue\n",
        "    \n",
        "    # Build feature matrix for prediction\n",
        "    # Need to create same structure as training (categorical + numerical)\n",
        "    X_roster = matched[['posteam', 'defteam', 'position'] + numerical_features].copy()\n",
        "    for col in numerical_features:\n",
        "        if col not in X_roster.columns:\n",
        "            X_roster[col] = 0\n",
        "    X_roster = X_roster.fillna(0)\n",
        "    \n",
        "    # Ensure column order matches training\n",
        "    X_roster = X_roster[categorical_features + numerical_features]\n",
        "    \n",
        "    # Transform and predict\n",
        "    X_roster_scaled = preprocessor.transform(X_roster)\n",
        "    \n",
        "    # Pure quantile predictions\n",
        "    matched['pred_10'] = models[0.10].predict(X_roster_scaled)\n",
        "    matched['pred_50'] = models[0.50].predict(X_roster_scaled)\n",
        "    matched['pred_90'] = models[0.90].predict(X_roster_scaled)\n",
        "    \n",
        "    # Fix quantile crossing\n",
        "    for idx in matched.index:\n",
        "        q10, q50, q90 = matched.loc[idx, 'pred_10'], matched.loc[idx, 'pred_50'], matched.loc[idx, 'pred_90']\n",
        "        sorted_quantiles = sorted([q10, q50, q90])\n",
        "        matched.loc[idx, 'pred_10'] = sorted_quantiles[0]\n",
        "        matched.loc[idx, 'pred_50'] = sorted_quantiles[1]\n",
        "        matched.loc[idx, 'pred_90'] = sorted_quantiles[2]\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\npred_10 = floor (90% chance to beat), pred_50 = median, pred_90 = ceiling\")\n",
        "    \n",
        "    output = matched[['player_name_espn', 'espn_proj_pts', 'actual_pts', 'pred_10', 'pred_50', 'pred_90']].copy()\n",
        "    output = output.rename(columns={'player_name_espn': 'player', 'espn_proj_pts': 'espn_proj', 'actual_pts': 'actual'})\n",
        "    \n",
        "    # Remove bye week players\n",
        "    bye_players = output[output['espn_proj'] == 0]['player'].tolist()\n",
        "    if bye_players:\n",
        "        print(f\"Excluding bye week/inactive players: {bye_players}\")\n",
        "    output = output[output['espn_proj'] > 0]\n",
        "    \n",
        "    output = output.sort_values('pred_50', ascending=False).reset_index(drop=True)\n",
        "    \n",
        "    # Round for readability\n",
        "    for col in ['pred_10', 'pred_50', 'pred_90', 'espn_proj']:\n",
        "        output[col] = output[col].round(1)\n",
        "    if 'actual' in output.columns:\n",
        "        output['actual'] = output['actual'].round(1)\n",
        "    \n",
        "    display(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Visualize Quantile Curves by Position\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_quantile_curves_by_position(df_predictions, position, n_bins=25):\n",
        "    \"\"\"\n",
        "    Make a quantile fan plot for a single position:\n",
        "      - x-axis: predicted median points (pred_50)\n",
        "      - lines: avg predicted 10/50/90 quantiles\n",
        "      - line: actual median in each bin (ground truth)\n",
        "      - background: scatter of individual weeks\n",
        "    \"\"\"\n",
        "    df_pos = df_predictions[df_predictions['position'] == position].copy()\n",
        "    if df_pos.empty:\n",
        "        print(f\"No rows found for position {position}\")\n",
        "        return\n",
        "\n",
        "    df_pos = df_pos.sort_values('pred_50')\n",
        "    nbins = min(n_bins, max(5, len(df_pos) // 20))\n",
        "    df_pos['pred_bin'] = pd.qcut(df_pos['pred_50'], q=nbins, duplicates='drop')\n",
        "\n",
        "    grouped = df_pos.groupby('pred_bin')\n",
        "    x = grouped['pred_50'].mean()\n",
        "    pred_q10 = grouped['pred_10'].mean()\n",
        "    pred_q50 = grouped['pred_50'].mean()\n",
        "    pred_q90 = grouped['pred_90'].mean()\n",
        "    actual_med = grouped['y_test'].median()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    ax.scatter(df_pos['pred_50'], df_pos['y_test'], alpha=0.15, s=15, label='Player-weeks')\n",
        "    ax.plot(x, pred_q10, linewidth=2, label='Predicted q=0.10')\n",
        "    ax.plot(x, pred_q50, linewidth=2, label='Predicted q=0.50')\n",
        "    ax.plot(x, pred_q90, linewidth=2, label='Predicted q=0.90')\n",
        "    ax.plot(x, actual_med, linewidth=2, linestyle='--', label='Actual median')\n",
        "\n",
        "    ax.set_xlabel('Predicted median fantasy points')\n",
        "    ax.set_ylabel('Fantasy points')\n",
        "    ax.set_title(f'Quantile curves by predicted points - {position}')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for each position\n",
        "plot_quantile_curves_by_position(df_predictions, position='QB')\n",
        "plot_quantile_curves_by_position(df_predictions, position='RB')\n",
        "plot_quantile_curves_by_position(df_predictions, position='WR/TE')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
